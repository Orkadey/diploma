## Криминалистически правильные методы извлечения дампов оперативной памяти виртуальных машин в контексте платформы виртуализации QEMU/KVM

## Введение

### Актуальность

В наши дни центральное место в ИТ-инфраструктурах всё чаще занимают облачные технологии, основанные на использовании платформ виртуализации. Виртуализация обеспечивает уровень абстракции через эмуляцию аппаратного обеспечения и содействует распределению ресурсов между несколькими гостевыми (виртуальными) машинами с различными операционными системами. Среда виртуализации состоит из нескольких виртуальных машин, монитора виртуальных машин (гипервизора) и аппаратных средств, регламентирующих доступ к ресурсам.

Как следствие, сами витруальные окружения и инфраструктура, лежащая в их основе, становятся потенциальной целью для атак злоумышленников [1]. Таким образом растет необходимость в создании систем безопасности для противостояния внешним и внутренним угрозам в виртуализованных средах.

Традиционные средства защиты операционных систем, такие как антивирусы, обеспечивают безопасность, используя перманентные агенты, установленные непосредственно внутри целевых систем. Главным недостатком такого подхода является уязвимость самих инструментов защиты перед вредоносным ПО, которое может, используя, скажем, нетривиальные комбинации эксплуатации неисправленных уязвимостей ядра и антиотладочные приемы, не только скрывать себя, но и блокировать программы-охранители.

В случае облачных технологий наиболее эффективным является подход к противодействию несанкционированной активности внутри целевых систем, основанный на механизме интроспекции виртуальных машин для мониторинга и анализа операционных систем извне (со стороны гипервизора). Основное преимущество здесь заключается в отсутствии доступа вредоносного ПО из гостевых машин к памяти хоста и, параллельно, полном доступе к "физической" памяти виртуальных машин для резидентов хост-системы. В этом случае мы говорим о возможности:

1. неинвазивного (т.е. без внедрения в гостевую память) изучения динамики оперативной памяти целевых систем;
2. создания проактивной (упреждающей) защиты целевых машин на основе высокочастотного мониторинга на предмет появления гарантированно опасных или аномальных активностей в памяти.

Наибольший интерес представляют методы, имеющих в своей основе принципы компьютерной криминалистики - дисциплины о раскрытии и расследовании преступлений, связанных с компьютерной информацией, о методах получения и исследовании доказательств, имеющих форму компьютерной информации и о применяемых для этого технических средств [2]. Далее такие методы будем обобщать понятием «криминалистический подход». Ключевыми понятиями в определении компьютерной криминалистики являются получение и исследование доказательств, при этом необходимо придерживаться определенных требований, соответствующих целям данной дисплины:

* сохранение целостности и достоверности данных;
* предотвращение «загрязнения» данных;
* полная и правильная документация;
* реализация правильных научных методов.

Тематикой данной работы является исключительно извлечение компьютерной информации. Можно выделять множество классов методов, применяемых для получения данных. Классификация зависит от выбираемого критерия, например, от типа памяти, с которой производится взаимодействие, таким образом, целью является

* энергозависимая память;
* энергонезависимая память.

Данная работа посвящена изучению и разработке методов первой группы. При этом получение дампа возможно с помощью

* программного обеспечения;
* использования аппаратных средств.

Методы в той или иной степени могут удовлетворять требованиям криминалистического подхода к целостности и предотвращния «загрязнения» данных. По данному критерию можно выделить следующие группы методов:

* криминалистически правильные;
* криминалистически сомнительные;
* криминалистически неправильные.

Криминалистически правильные – методы, полностью удовлетворяющие требованиям криминалистического подхода. Применение криминалистически сомнительных методов не оказывает влияния на данные, однако не гарантирует сохранения целостности, т. е. получение данных не является атомарной операцией. Последняя группа подразумевает методы, непосредственно изменяющих данные, что нарушает их целостность и/или достоверность. В дальнейшем будем рассматривать только два первых класса методов.

Большая часть подходов к получению дампов оперативной памяти подразумевают использование дополнительно установленного программного обеспечения непосредственно на стороне целевой системы. Такие методы будем называть *инвазивными*. Соответственно выделим два класса по данному критерию:

* инвазивные;
* неинвазивные.

Необходимо также учитывать возможность непрерывной работы целевой системы при ее исследовании, в частности, при получении дампа оперативной памяти. В этом случае методы разделяют на:

* требующие остановки целевой системы;
* не требующие остановки целевой системы.

Примером неинвазивного, криминалистически правильного метода, требующего остановки системы является стандартная функция гипервизора QEMU/KVM pmemsave. При её использовании гипервизор останавливает виртуальную машину, после чего снимает дамп оперативной памяти. Подробнее различные методы будут рассмотрены в **главе ХХХ**.

### Цель работы

Данная работа посвящена разработке криминалистически правильных и методов получения дампов оперативной памяти виртуальных машин, не требующих остановки целевых систем.

### Задачи работы

Цель работы предполагает решение следующих задач:

1. Обзор архитектуры QEMU/KVM;
2. Обзор методов получения дампов оперативной памяти;
3. Разработка семейства криминалистически правильных методов получения дампов памяти виртуальных машин; реализация эталонного метода;
4. Проведение вычислительного эксперимента для верификации разработанных методов.

## Глава 1. Платформа виртуализации QEMU/KVM

В данной главе описываются общие принципы и особенности работы платформы виртуализации QEMU/KVM.

### 1.1. Гипервизор

Гипервизор (монитор виртуальных машин) - это программное или программно-аппаратное решение, позволяющее организовать на одном компьютере одновременную работу нескольких операционных систем. Существуют два типа гипервизоров:

* автономный (тип 1, bare-metal) — гипервизор, имеющий свои встроенные драйверы устройств, модели драйверов и планировщик, поэтому не зависящий от базовой ОС;
* на основе базовой ОС (тип 2, hosted) — гипервизор, работающий в одном кольце с ядром основной ОС (кольцо 0), т. е. выполняющийся как процесс.

Однако, разделение не всегда четкое, например KVM и BHyVe являются модулями ядра, конвертирующими операционнную систему хоста в гипервизор первого типа.

Среди множества гипервизоров выделим основные для различных операционных систем.

В операционных системах семейства Windows NT в настоящее время лидирующее место занимает гипервизор первого типа Hyper-V. Hyper-V разделяет ресурсы хоста между виртуальными машинами, используя концепцию *раздела* (partition). Каждый экземпляр гипервизора должен иметь как минимум один *родительский раздел*, с запущенной операционной системой Windows Server (2008 или позднее). Сервер виртуализации исполняется в родительском разделе и имеет прямой доступ к аппаратным ресурсам хост-системы. Родительский раздел порождает дочерние разделы, в которых и исполняются гостевые ОС. Дочерний раздел также может породить собственные дочерние разделы. Родительский раздел создает дочерние при помощи API, представленного Hyper-V. Виртуальные утройства данного гипервизора поддерживают технологию Enlightened I/O, являющейся реализацией высокоуровневых протоколов таких как SCSI, для прямого доступа к VMBus, что позволяет параллельно взаимодействовать с любыми уровнями эмуляции устройств.

Разработчиками FreeBSD развивается гипервизор BHyVe, опирающийся на поддержку технологий аппаратной виртуализации Intel VT-x и AMD-V. Также данным гипервизором используются технологии EPT (Extended Page Table) и VirtIO.

В операционных системах семейства Linux большой популярностью пользуется гипервизор второго типа QEMU (от "Quick EMUlator"). Такие продукты, как VirtualBox, Xen имеют базовые составляющие QEMU [3]. В настоящее время QEMU обычно используется совместно с модулем KVM (Kernel-based Virtual Machine), что обеспечивает уровень производительности близкий к "чистому" аппаратному.

### 1.2. Платформа виртуализации QEMU/KVM

QEMU — свободный программный комплекс с открытым исходным кодом для эмуляции и виртуализации аппаратного обеспечения. Это означает, что QEMU обеспечивает возможность работы целевой операционной системы и всех ее приложений в виртуальной машине. QEMU может использоваться на разных операционных системах, таких как Linux, Windows и Mac OS X. Кроме главной цели, которой является запуск одной операционной системы на другой, QEMU может быть использован для отладки, поскольку виртуальная машина может быть легко остановлена и ее состояние может быть сохранено, проанализировано и восстановлено.

KVM — это программное решение, обеспечивающее виртуализацию в среде Linux, которая поддерживает аппаратную виртуализацию на базе Intel VT-x либо AMD-V x86-совместимых процессоров. Программное обеспечение KVM состоит из загружаемого модуля ядра (называемого kvm.ko), предоставляющего базовый сервис виртуализации, процессорно-специфического загружаемого модуля kvm-amd.ko либо kvm-intel.ko, и компонентов пользовательского режима (модифицированного QEMU). Все компоненты KVM являются программным обеспечением с открытым исходным кодом.

### 1.3. Архитектура KVM

Сам по себе KVM не выполняет эмуляции. Вместо этого программа, работающая в пространстве пользователя (в нашем случае - QEMU), использует интерфейс /dev/kvm для настройки адресного пространства гостя виртуальной машины, через него же эмулирует устройства ввода-вывода, видеоадаптер и другое оборудование. Иными словами, KVM использует ядро Linux как автономый гипервизор (тип 1), и на хосте выполняется не код KVM, а код ядра Linux и модуля KVM, являющегося частью ядра Linux. Такой подход получает преимущество над гипервизорами, решающие задачи прямого аппаратного и ресурсного управления, поскольку данные задачи уже решены в ядре операционной системы. Linux представляет из себя монолитнию систему с отдельными элементами модульного ядра, поэтому предоставляет идеальную среду для построения гипервизора. Далее мы более подробно рассмотрим механизм работы инфраструктуры KVM и взаимодействие с ней QEMU.

#### 1.3.1. Технологии аппаратной виртуализации

Эффективность подхода, основанного на использовании KVM, обеспечивается поддержкой технологий аппаратной виртуализации, которые добавляют режим выполнения, позволяющий организовать работу гостевых машин без необходимости предоставления им полного доступа к памяти и ресурсам. Для понимания принципов работы KVM предоставим общую концепцию технологий Intel VT-x и AMD-V.

Intel VT-x и AMD-V можно рассматривать как функции, которые переключают обработку на гипервизор при обнаружении центральным процессором sensitive-инструкций (sensitive instructions). Существует два типа таких инструкций:

1. Изменяющие состояние системных ресурсов (англ. "control-sensitive instructions");
2. Поведение или результат которых зависит от состояния ресурсов (англ. "behaviour-sensitive instructions").

__Инструкции первого типа, выполненые на виртуальной машине, задействуют операции на физической машине, в то время как инструкции второго типа показывают, что они были выполнены на виртуальной машине, поскольку результаты отличаются от тех, как если бы они были выполнены на физической машине.__

__Если программа пытается выполнить инструкции в гостевой системе без вмешательства, это вызовет проблемы на стороне гипервизора и гостевой системы. Отсюда возникает необходимость в детектировании процессором начала выполнения инструкции и в управлении гипервизором для ее выполнения на стороне программы. [ПЕРЕФОРМУЛИРОВАТЬ!]__

CPU архитектуры x86 изначально не были предназначены для виртуализации, поэтому существовали инструкции, которые гостевая машина пыталась выполнить, но при этом они не могли быть выявлены посредством CPU, и, в результате, гипервизор не мог выполнить инструкции на стороне гостевой системы. Для решения данной проблемы были разработаны технологии Intel VT-x и AMD-V. Они добавляют новые режимы выполнения для процессора и переключения между этими режимами. Этими двумя режимами являются VMX root operation и VMX non-root operation (VMX от "Virtual Machine eXecution"). Последний режим является гостевым режимом исполнения. __Если производится попытка исполнения базовых команд, CPU обнаруживает попытку и переключает выполнение в режим VMX root operation, который является режимом выполнения для гипервизора. [ПЕРЕФОРМУЛИРОВАТЬ!]__ Это переключение, называемое "VM Exit", сигнализирует передачу контроля гипервизору для выполнения __базовых (???)__ инструкций на стороне гостевой системы [6].

Технология Intel VT-x, аналогично AMD-V, вводит две новых инструкции процессора - VMLAUNCH и VMRESUME - соответственно вход и переключение в режим VMX non-root operation, которое называется VM Entry. Главная роль KVM заключается в обработке переключения VM Exit и VM Entry.

![KVM Intel](https://github.com/Orkadey/diploma/blob/master/pictures/kvm_intel.png)

#### 1.3.2. Использование KVM в QEMU

KVM не может самостоятельно создавать виртуальные машины. На хосте эмуляция устройств производится процессом пользовательского пространства qemu-kvm. Это позволяет модулю ядра сфокусироваться на аспектах наиболее важных в плане производительности, в то время как __эмуляция устройств пользовательского пространства эмулирует аппаратное обеспечение (???)__ в изолированном процессе вне ядра системы. Безопасность обеспечивается технологией sVirt, блокирующей процесс qemu-kvm с помощью SELinux MAC (Mandatory Access Control) таким образом, чтобы ему предоставлялся доступ только к необходимиым файлам и ресурсам.

Поскольку QEMU является программным решением эмуляции, он интерпретирует и выполняет __команды (или инструкции???)__ CPU по одной, что означает ограниченность быстродействия. Однако быстродействие можно значительно увеличить, если учитывать три условия:

1. целевая команда может быть напрямую выполнена на CPU;
2. команда может быть передана без модификации центральному процессору для выполнения в режиме VMX non-root opeation;
3. целевая инструкция, которая не может быть выполнена напрямую, может быть идентифицирована и передана QEMU для обработки.

__Разработка KVM была основана на реализации этой концепции. Применение этой идеи подразумевает создание виртуальных машин, максимизируя использование ресурсов существующего решения с минимальными модификациями. [ПЕРЕФОРМУЛИРОВАТЬ!]__

Последовательность выполнения QEMU/KVM показана на **рисунке** __[ПЕРЕДЕЛАТЬ В ПОШАГОВЫЙ АЛГОРИТМ + осбудить комментарий к рисунку]__. 
__Шаг 0:__ __Модуль ядра KVM (модуль ядра Linux или модуль ядра KVM???)__ создает файл `/dev/kvm`. Этот файл позволяет QEMU передавать различные запросы модулю ядра KVM для выполения функций гипервизора. Когда процесс QEMU запускается для создания гостевой системы, он повторно осуществляет системный вызов `ioctl()`, указывая файл `/dev/kvm`, передавая соответсвующий файловый дескриптор. Когда наступает время начала работы гостевой системы, QEMU снова вызывает `ioctl()` для отправки команды запуска гостевой системы модулю KVM (шаг 1). В свою очередь модуль ядра выполняет VM Entry (шаг 2) и начинает работу гостевой системы. Далее, когда гостевая система собирается выполнить базовую команду, выполняется VM Exit (шаг 3), и KVM определяет причину выхода. Если для выполнения задачи ввода/вывода или другой задачи необходимо вмешательство QEMU, контроль передается QEMU процессу (шаг 4), и QEMU выполняет задачу. По окончании выполнения QEMU снова осуществляет вызов `ioctl()` и запрашивает KVM о продолжении работы гостевой системы (шаг 1). Такая последовательность выполнения повторяется на протяжении эмуляции виртуальной машины.

![KVM QEMU](https://github.com/Orkadey/diploma/blob/master/pictures/KVM_QEMU_and_KVM.png)

### 1.4. Архитектура QEMU

Создание виртуальной машины начинается с запуска процесса QEMU. Для каждой виртуальной машины создается отдельный процесс, множество гостевых машин работают изолированно друг от друга. В момент выключения гостевой операционной системы процесс QEMU завершается, но перезагрузка может быть произведена без завершения процесса и старта нового.

Непосредственно после запуска процесса гостевой машине выделяется блок виртуальной памяти (адресная арена) хоста. В основном, процесс QEMU/KVM выполняется как большинство других процессов операционной системы Linux. Память для процесса выделяется вызовом стандартных функций `malloc()`, `mmap()`. Например, если гостевая система создается с размером памяти равной 1 ГиБ, QEMU/KVM вызовет `malloc(1<<30)`, выделяя 1 ГиБ виртуального адресного пространства хоста. Однако, как и в других случаях вызова функции `malloc()`, в действительности физическая память не выделяется до тех пор, пока не будет задействована. Как только гостевая операционная система начинает свою работу, она видит регион памяти выделенной функцией `malloc()` как свою физическую память. Если ядро гостевой системы обращается к тому, что находится, например, по адресу 0x0, оно увидит первую страницу выделенной памяти. QEMU поддерживает порядок записи байтов от старшего к младшему (big-endian) и от младшего к старшему (little-endian). Поддержка обеспечивается вспомогательными функциями, без использования прямого доступа к оперативной памяти гостевой машины.

Запущенная виртуальная машина располагается целиком в адресном пространстве процесса QEMU, следовательно детальное представление о процессах, выполняющихся в гостевой системе не доступны хосту.

Работа гостевой операционной системы состоит из выполнения разнородного гостевого кода, включая обработку таймеров, выполнение операций ввода/вывода, обработку комманд со стороны QEMU и т. п. Чтобы безопасно распределять программно-аппаратные ресурсы, не останавливая работу гостевой ОС даже в случае длительного ожидания результатов комманд (например, дисковых операций ввода/вывода) гипервизору требуется специальная архитектура.

Выбор можно сделать из трёх программных архитектур:

* Параллельная: разделяет работу по процессам или потокам, которые могут выполняться одновременно;
* Событийная: реагирует на события внутри главного цикла, из которого события передаются в обработчики событий. Примером служат системные вызовы `select(2)` и `poll(2)` для ожидания на нескольких файловых дискрипторах;
* Гибридная: сочетает глобальный цикл событий и обработчики определенных событий, запускаемые в отдельных потоках выполнения.

QEMU использует последнюю, где в качестве глобального цикла событий выступает `main_loop_wait()` и выполняет следующие задачи:

1. Ожидает, пока файловые дескрипторы не станут доступными для чтения или записи. (Именованные каналы, сокеты, файлы и другие ресурсы являются файловыми дескрипторами, поэтому данная задача играет важную роль.);
2. Запускает таймеры и низкоприоритетные обработчики прерываний.

Когда дескриптор файла становится доступным, время таймера истекает или запускается низкоприоритетный обработчик прерывания, цикл событий вызывает функцию обратного вызова. Каждая функция подчиняется двум правилам:

1. никакой другой код QEMU не выполняется в то же время, благодаря чему отпадает необходимость в синхронизации. Функции обратного вызова выполняются последовательно и атомарно по отношению к другому коду.
2. не выполняются блокирующие системные вызовы или операции, требующие длительной обработки, поскольку цикл обработки событий ждет возврата из функции обратного вызова, таким образом нарушение данного правила приводит к приостановке гостевой системы.

Фактически, однако, некоторые операции ввода/вывода не имеют неблокирующего эквивалента. Кроме того, существуют некоторые операции, требующие длительной обработки, которые не могут быть разбиты на несколько функций обратного вызова. В таких случаях используются дополнительные рабочие потоки, выполняющие данные задачи вне ядра QEMU. В момент, когда ядро QEMU должно быть уведомлено рабочим потоком, в цикл обработки событий добавляется именованный канал или фаловый дескриптор `qemu_eventfd()`. Рабочий поток производит запись в файловый дескриптор и, когда он становится читаемым, цикл обработки событий вызывает функцию обратного вызова. В дополнение к этому необходимо использование сигналов для гарантированной работы цикла обработки событий.

__Важным аспектом в работе QEMU является возможность выполнения кода гостевой системы [Спасибо, Кэп! ПЕРЕФОРМУЛИРОВАТЬ ИЛИ УБРАТЬ!]__. Существуют два механизма выполнения гостевого кода:
1. Обеспечиваемый KVM и описанный ранее.
2. TCG (Tiny Code Generator), который использует динамическую двоичную трансляцию, и применяющийся в случае, когда KVM не используется.

Выполнение гостевого кода приводит к передаче контроля управления выполнения кода с хоста гостевой системе. Пока поток исполняет код гостевой системы, он не может одновременно находиться в главном цикле обработки событий. __В крайних случаях гостевая система теоретически может сохранять контроль неограниченное время, что привело бы к зависанию процесса. [ПЕРЕФОРМУЛИРОВАТЬ!]__ Для устранения данной проблемы используются сигналы. Сигналы прерывают работу текущего потока и вызывает функцию обработки сигналов. Это позволяет QEMU выйти, перенаправить контроль управления и вернуться в главный цикл обработки событий. Результатом этого является то, что новые события могут быть обнаружены не сразу, если QEMU выполняет гостевой код. Чаще всего QEMU в конечном счете обрабатывает события, но эта задержка сама по себе является проблемой в плане быстродействия. С этой целью таймеры, завершения оперций ввода/вывода и уведомления рабочих потоков ядру QEMU используют сигналы, для обеспечения немедленного выполнения цикла обработки событий.

Изначально QEMU поддерживал единственный поток, который исполнял гостевой код и цикл обработки событий. Поток QEMU исполняет код гостевой машины до момента появления исключения, либо сигнал возвращает контроль. Далее он выполняет одну итерацию цикла обработки событий без блокирования в `select()`. Далее поток возвращается в гостевой код и повторяет действия до тех пор пока процесс существует.

В текущей архитектуре QEMU каждая виртуальная машина имеет VCPU-поток для каждого эмулируемого процессора. VCPU-потоки могут испольнять код параллельно, в то время как `iothread` выполняет цикл обработки событий для обработки пакетных операций сетевых соединений, дисковых операций ввода/вывода и т. п. Правило, по которому код QEMU никогда не выполняется одновременно осуществляется через глобальную блокировку. Большую часть времени VCPU исполняют гостевой код и не нуждаются в глобальной блокировке, большую часть времени `iothread` остается заблокированным в `select()` и также не нуждается в глобальной блокировке. Стоит отметить, что TCG не обеспечивает потоковую безопасность, только KVM может использовать преимущества данной архитектуры.

![QEMU QEMU_KVM](https://github.com/Orkadey/diploma/blob/master/pictures/QEMUKVM_arch.png)

#### 1.4.1. Механизмы оптимизации

Поскольку изначально центральные процессоры семейства x86 не имели оптимизации для виртуализации, всякий раз когда гостевая система изменяла свои *таблици страниц* (англ. page table), было необходимо вмешательство хоста. Хост должен был проверять, что значения, которые гостевая система помещает в таблицы страниц являются корректными и обращаются только к доступным областям памяти. Это осуществлялось при помощи двух механизмов:

1. Механизма *теневых таблиц страниц* (англ. shadow page tables). ~~Набор таблиц страниц, используемых аппаратным обеспечением виртуализации существовало отдельно от таблиц страниц, которые гостевая система принимала за используемые. Сначала гостевая система проводила изменения в своих таблицах страниц. Далее, хост, обнаружая данные изменения, проверял их и изменял реальные таблицы страниц, используемые аппаратным обеспечением. Соответственно, гостевой системе не позволено напрямую управлять таблицами страниц, используемых аппаратным обеспечением.~~ __[Адаптировать описание отсюда: http://stackoverflow.com/questions/9832140/what-exactly-do-shadow-page-tables-for-vmms-do]__

2. Расширения VMX, позволяющее хосту перехватывать попытки гостевой системы (пере)установить значение регистра, указывающего на базовую таблицу страниц (CR3).

Приведенная техника обеспечивает надежную работу виртуальных машин, однако имеет серьезные недостатки с точки зрения быстродействия. Единичное обращение к гостевой странице может занять до 25-и последовательных обращений, что означает высокие затраты по времени выполнения операций. Корнем проблемы является то, что каждое обращение в общем случае задействует таблицы страниц как гостевой системы, так и хоста. Двухуровневая часть появляется из-за необходимости гостевых таблиц страниц "самостоятельно" использовать таблицы страниц хоста. Таким образом, для хоста проверка и содержание теневых таблиц страниц могут оказаться затратными.

Компании AMD и Intel нашли решение этих проблем и пришли к похожим ответам - механизмам EPT (Extended Page Table) и NPT (Nested Page Table). Они устанавливают набор структур, распознаваемых аппаратным обеспечением, которые могут быстро переводить физические адреса гостевой системы в физические адреса хоста "без" необходимости обращения к табицам страниц хоста, что устраняет двухуровневый проход по таблицам.

Проблема данного подхода заключается в том, что таблицы страниц хоста это то, что используется для принужденных действий. Например, если страница будет выгружена из памяти хоста, необходимо скоординировать данное изменение в структурах EPT/NPT.

Программным решением является `mmu_notifires` в операционной системе Linux. Поскольку память QEMU/KVM является, с позиции ядра, частью памяти хоста, ядро может свободно использовать подкачку страниц, перемещать области памяти и освобождать память. Однако, перед тем как страницы передаются обратно хосту для использования, гостевая машина получает сообщение о действиях хоста. Тогда гостевая система удаляет страницу из теневых таблиц страниц или структур EPT/NPT. После окончания необходимых операций со стороны гостевой системы, хост может использовать страницы памяти.

Пример жизненного цикла страницы памяти:

Выделение памяти:

1. QEMU вызывает `malloc()` и выделяет память в виртуальном адресном пространстве для страницы, но без соответствующей физической памяти;
2. гостевая машина обращается к памяти, как к физической, однако запрос перехватывается хостом, поскольку физическая память не выделена;
3. ядро операционной системы хоста обнаруживает page fault, вызывает `do_page_fault()` в соответствующей области памяти и выделяет физическую память;
4. ядро хоста создает запись таблицы страниц (`pte_t`) для создания связи между физическим и виртуальным адресом, создает `rmap` записи, помещает их в LRU и производит другие необходимые действия;
5. вызывается `mmu_notifier_change_pte()`, что позволяет KVM создавать EPT/NPT запись для новой страницы;
6. хост возвращается из page fault и гостевая система продолжает работу.

Использование подкачки страниц:

1. ядро хоста использует rmap струткуры для нахождения расположения страницы в VMA (`vm_area_struct`);
2. ядро хоста читает структуру `mm_struct`, соостветсвующую VMA и проходит по таблицам страниц операционной системы Linux для нахождения `pte_t` страницы;
3. ядро хоста выгружает страницу и убирает запись pte_t (предполагаем, что страница была использована только в одном месте);
4. ядро хоста вызывает `mmu_notifier_invalidate_page()`, что находит и убирает соответсвующую странице запись из структур EPT/NPT;
5. страница освобождается, далее любое обращение к странице будет перехватываться хостом (случай выше).

Помимо механизма EPT, оптимизация работы QEMU/KVM также обеспечивается некоторыми другими функциями:

1. VT-d (Intel Virtualization Technology for Directed I/O). VT-d является механизмом трансляции адресов для утройств ввода/вывода (IOMMU). Он предоставляет таблицы перевода адресов для DMA перемещения данных устройств, а также обеспечиавет маршрутизацию и изолированность аппаратных прерываний.
2. virtio. Устройства виртуальной машины обычно создаются и управляются с помощью эмуляции устройств QEMU. Такая эмуляция требует больших затрат ресурсов, из-за чего сильно уменьшается быстродействие. Для решения данной проблемы был создан механизм под названием "virtio". Virtio предоставляет буфер, доступный как гостевой системе так и QEMU. Используя данный буфер, обработка ввода/вывода для нескольких фрагментов данных может быть произведена совместно, уменьшая тем затраты ресурсов, связанных с эмуляцией, производимой QEMU. Обращение к механизму может быть произведено со стороны гостевой системы как к устройству virtio PCI.
3. KSM (Kernel Same-page Merging). На различных виртуальных машинах одной физической машины иногда могут работать одинаковые операционные системы и одинаковые приложения. В таком случае у данных машин велика вероятность наличия одинаковых областей памяти. Объединения таких областей в одну позволило бы уменьшить использование оперативной памяти. Такая концепция была реализована и добавлена в ядро Linux для KVM. KSM использует поток ядра "ksmd" для периодического мониторинга использования памяти процессов и автоматического слияния дублирующихся страниц в одну. В идеале, все страницы памяти должны быть проверены для нахождения дубликатов, но постоянная проверка была бы неэффективной. С этой целью Linux определяет кандидатов для KSM используя параметр для системного вызова `madvise()`, т. е. параметр "advice". QEMU использует эту функцию когда выделяет память для гостевой системы.

### 1.5. Управление и мониторинг виртуальных машин

Важной частью QEMU/KVM является осуществление управления и мониторинга виртуальных машин. Инструменты управления должны следить и получать доступ к гостевым системам, которые могут быть запущены на удаленных физических машинах, либо локально. Это осуществляется с помощью множества API и утилит, включающих приложения для манипулирования гостевыми машинами и автоматизирования задач управления. Стандартный монитор QEMU предоставляет командный интерфейс для передачи команд эмулятору. Возможно осуществление следующих операций:

* приостановка/продолжение работы виртуальной машины, сохранение и загрузка ее состояния;
* получение информации о состоянии виртуальной машине без внешнего отладчика;
* вставка/удаление образов съемных носителей;
* получение дампов памяти виртуальной машины;
* добавление/удаление устройств;
* миграция виртуальных машин - перемещение гостевой фиртуальной машины с одного QEMU процесса на другой.

Полный список приведен в официальной документации. Кроме операций, осуществляемых непосредственно QEMU, KVM __предоставляет особенности [Что???]__ паравиртуализированных драйверов балунинга памяти (англ. memory ballooning), сетевых соединений и хранилищ данных, которые увеличивают эффективность ввода/вывода и позволяют управлять количеством доступной оперативной памяти, выделяемой гостевой машине.

Для взаимодействия стороннего программного обеспечения с QEMU используется QMP (QEMU Machine Protocol) - протокол, основанный на формате JSON. Наиболее популярной библиотекой для взаимодействия с QEMU является libvirt, которая предоставляет функции и утилиты командной строки для разработки приложений и написания скриптов. На каждом хосте работает демон `libvirtd`, который обеспечивает безопасный API удаленного управления, но также может быть сконфигурирован для исключительно локального использования. Демон `libvirtd` сохраняет настройки гостевой системы и является центральной точкой для установления сетевых соединений и хранилищ.

Большая часть администрирования происходит с использованием API библиотеки libvirt, в особенности инструмента virsh, который предоставляет оперции управления гостевой системой и хостом. Ту же цель имеет графический инструмент virt-manager. Сторонние инструменты такие как OpenStack могут быть использованы для высокоуровнего управления датацентрами и, чаще всего, интегрируемы с libvirt.

Далее более детально рассмотрим особенности организации памяти в QEMU, а также функционал, существенно влияющий на структуру памяти гостевоц системы, поскольку данные аспекты могут непосредственным образом повлиять на решение поставленных в данной работе задач.

#### 1.5.1. API управления памятью, структура памяти

API управления памятью моделирует RAM, шины ввода/вывода и контролеры гостевой машины. Он позволяет моделировать:

* RAM;
* MMIO (memory-mapped I/O);
* контролеры памяти, которые могут динамически перенаправлять регионы физической памяти по различным направлениям.

Модель памяти обеспечивает поддержку:

* отслеживания изменений оперативной памяти гостевой системы;
* подготовки объединенной памяти (coalesced memory) для KVM;
* установки `ioeventfd` регионов для KVM.

Структура памяти представляет из себя ациклический граф объектов MemoryRegion. Листьями являются регионы оперативной памяти и MMIO, остальные узлы представляют шины, контролеры памяти и перенаправленные регионы памяти. В дополнение к объектам MemoryRegion, API предоставляет объекты AddressSpace для каждого корня и промежуточных объектов MemoryRegion. Они отображают память так, как она видима с точки зрения CPU или периферийных устройств.

В данной модели существуют четыре типа регионов памяти (все являются типом MemoryRegion):

* RAM. RAM является блоком оперативной памяти хоста, которая может быть использована гостевой системой;
* MMIO. Блоки памяти гостевой машины, которые имплементированы функциями обратного вызова хоста, т. е. каждая операция чтения и записи означает вызов функции обратного вызова на хосте;
* Контейнер (container). Контейнер включает в себя другие регионы памяти, каждый из которых располагается по разным адресам. Контейнеры используются для группирования нескольких регионов в один, например, PCI BAR может быть представлен в виде объединения регионов RAM и MMIO. Регионы составляющие контейнер обычно не перекрываются. В некоторых случаях удобным оказывается использование перекрывающихся регионов, например, для котролера, который может перекрывать субрегионы RAM регионами MMIO или ROM;
* Alias. Является секцией региона памяти, регионы alias позволяют разделить регион памяти на непересекающиеся регионы. Примером служат банки памяти, когда адресное пространство гостевой системы меньше, чем количество адресованной оперативной памяти. Данный тип может указывать на регион памяти любого типа, включая сам тип alias, но не на себя.

Таже допустимо добавлять субрегионы к регионам, которые не являются контейнерами (MMIO, RAM или ROM). Это означает, что регион будет вести себя как контейнер, за исключением случая, когда любые адреса внутри региона, которые не востребованы субрегионом, обрабатываются самими контейнерами. Однако такой эффект достижим и с помощью контейнера, один из субрегионов которого является регионом с низким приоритетом, покрывающим весь диапазон адресов.

Регионы создаются одной из функций `memory_region_init*()` и прикрепляются к обекту, который действует как их родитель/владелец. QEMU гарантирует, что объект-владелец остается активным до тех пор, пока регион остается видимым для гостевой системы, либо пока используется виртуальным CPU или другим утройством. Например, объект-владелец будет присутствовать между операцией address_space_mmap и соответсвующей операцией address_space_munmap. После создания регион может быть добавлен к адресному пространству или контейнеру с помощью функции `memory_region_add_subregion()` и удален функцией `memory_region_del_subregion()`.

Ядро памяти использует следующий набор правил для выбора региона при доступе к определенному адресу:

* все подрегионы корневого региона сопоставляются с адресом в порядке убывания приоритета;
  * если адрес лежит вне региона адрес/размер, субрегион отбрасывается;
  * если субрегион является листом (RAM, MMIO), поиск прекращается, возвращая субрегион;
  * если субрегион является контейнером, алгоритм выполняется заново для данного субрегиона, адрес регулируется значением адреса субрегиона;
  * если субрегион - alias, поиск продолжается в данном субрегионе, адрес регулируется значением адреса субрегиона;
  * если рекурсивный поиск внутри контейнера или субрегиона alias не дает результат из-за наличия "дыры" в покрытии контейнером его адресного диапазона, тогда, в случае если контейнер опирается на собственный MMIO или RAM, поиск прекращается, возвращая контейнер, иначе поиск продолжается в субрегионах в порядке приоритета;
* если никакой из субрегионов не соответствует адресу, поиск прекращается с результатом отсутствия соответствия.

Пример структуры памяти:

```
system_memory: container@0-2^48-1
 |
 +---- lomem: alias@0-0xdfffffff ---> #ram (0-0xdfffffff)
 |
 +---- himem: alias@0x100000000-0x11fffffff ---> #ram (0xe0000000-0xffffffff)
 |
 +---- vga-window: alias@0xa0000-0xbfffff ---> #pci (0xa0000-0xbffff)
 |      (prio 1)
 |
 +---- pci-hole: alias@0xe0000000-0xffffffff ---> #pci (0xe0000000-0xffffffff)

pci (0-2^32-1)
 |
 +--- vga-area: container@0xa0000-0xbffff
 |      |
 |      +--- alias@0x00000-0x7fff  ---> #vram (0x010000-0x017fff)
 |      |
 |      +--- alias@0x08000-0xffff  ---> #vram (0x020000-0x027fff)
 |
 +---- vram: ram@0xe1000000-0xe1ffffff
 |
 +---- vga-mmio: mmio@0xe2000000-0xe200ffff

ram: ram@0x00000000-0xffffffff

```
Блок оперативной памяти размером 4ГиБ отображен на адресное пространство системы через два региона типа alias:

* "lomem" - отображение 1:1 первых 3.5 ГиБ;
* "himem" - отображение последних 0.5 ГиБ по адресу 4 ГиБ (0x100000000).

Такое отображение оставляет 0.5 ГиБ для так называемой "PCI hole", которая позволяет 32-х битной PCI шине существовать в системе с 4 ГиБ памяти. Контролер памяти направляет адреса в диапазоне 640K-780K в адресное пространство PCI. Это моделируется при помощи региона "vga-window", отображенного с большим приоритетом таким образом, что затеняет RAM в том же диапазоне адресов. Данный регион может быть устранен при програмировании контролера памяти, путем устранения alias-региона и выявления нижележащего RAM.

Адресное пространство PCI не является прямым потомком системного адресного пространства, поскольку видимыми должны быть только его части. У данного адресного пространства есть два субрегиона: "vga-area" моделирует устаревшее vga окно (vga window) и занимает два банка памяти размером 32 KиБ указывающих на две секции кадрового буфера (framebuffer). В дополнение, по адресу e1000000 vram отображен как BAR и дополнительный BAR после него, содержащий MMIO регистры.

#### 1.5.2. Балунинг памяти

Балунинг памяти (англ. memory ballooning) является способом распределения нагрузки за счет перераспределения оперативной памяти гостевой системы без приостановки или перезапуска гостевой системы. Внутри гостевой системы имеется драйвер ядра под названием virtio_balloon. Поведение этого драйвера похоже на поведение процесса, увеличивающего или уменьшающего объем использумеой оперативной памяти. Балунинг является __операцией взаимодействия [???]__ гипервизора и гостевой системы и осуществляется по следующему алгоритму:

1. гипервизор посылает запрос гостевой операционной системе для возвращения некоторого количества памяти гипервизору;
2. драйвер virtio_balloon получает запрос от гипервизора;
3. драйвер virtio_balloon расширяет область памяти (balloon) внутри гостевой операционной системы:
  * гостевая операционная система не может использовать память внутри выделенной области;
  * драйвер virtio_balloon пытается выполнить запрос гипервизора. Однако, существует возможность того, что действия драйвера не смогут полностью  удовлетворить требованиям запроса. Например, когда приложение в гостевой операционной системе занимает большое количество памяти, драйвер virtio_balloon может не найти достаточное количество доступной памяти, в таком случае драйвер выделяет максимально возможное количество памяти;
4. гостевая операционная система возвращает область памяти в гипервизор;
5. гипервизор распределяет возвращенную память по своему усмотрению;
6. если память из выделенной области освобождается, она может быть возвращена гостевой системе:
  * гипервизор посылает запрос драйверу virtio_balloon гостевой операционной системы;
  * запрос передает команду сжатия области памяти;
  * память из выделенной области становится доступной остевой системе.

У технологии балунинга есть свои недостатки и преимущества. С одной стороны требование установки модуля ядра (драйвера) в гостевой системе может негативно влиять на быстродействие и формировать дополнительный вектор атак для вредоносного кода внутри гостевых операционных систем. С другой стороны, балунинг позволяет организовывать виртуализацию даже на хостах со скромными объёмами оперативной памяти. Важным свойством также является то, что балунинг виден только гипервизору и гостевой машине, а с точки зрения хост-системы ничего не изменяется.

#### 1.5.3. Горячее добавление памяти

Горячее добавление памяти (англ. memory hotplug) - возможность добавления оперативной памяти гостевой системе, не требуя ее перезагрузки. Данная возможность была добавлена в QEMU сравительно недавно, начиная с версии 2.1.0. Поддержка горячего изъятия памяти в настоящее время недоступно. Общий принцип работы данного механизма заключается в том, что операционной системе становится доступно новый диапозон адресов памяти. Далее операционная система создает соответствующие адресные структуры для возможности получения доступа и использования добавленной памяти.

Для получения возможности горячего добавления памяти необходимо заранее определить количество слотов добавляемой памяти и максимальное возможное количество оперативной памяти гостевой системы, например:
```
qemu [...] 1G,slots=3,maxmem=4G
```
Создается виртуальная машина с 1 Гиб оперативной памяти и тремя слотами для горячего добавления памяти.

Для непосредственно добавления памяти используются две команды:

* "object_add": создает объект, соответствующий добавляемой памяти;
* "device_add": создает pc-dimm устройство и вставляет его в первый пустой слот.

Кроме добавления оперативной памяти, QEMU может использовать файлы как объекты, соответсвующие добавляемой памяти. Такая особенность полезна при использования страниц большого размера (hugepages) в операционной системе Linux.

## Глава 2. Методы получения дампов оперативной памяти

Кратко разберем некоторые существующие методы получения дампов и подробнее рассмотрим те из них, которые представляют наибольший интерес.

### 2.1. Методы получения дампов физических машин

Tribble - PCI карта расширения (PCI expansion card), которая может использоваться для получения содержимого физической памяти. Процесс получения дампа включает остановку процессора (если возможно), копирование содержимого оперативной памяти с ведением журнала и подсчетом контрольной суммы копируемых блоков, записывемых в журнал. Данный метод имеет свои сильные стороны и ограничения. Как аппаратное средство, Tribble может получать доступ к памяти без установки дополнительного программного обеспечения. Однако данное утройство не получило распространения из-за необходимости предварительной физической установки, что является серьезным недостатком для криминалистических исследований [7].

Другим решением с использованием аппаратных средств являются устройства Firewire, которые используют прямой доступ к памяти (Direct Memory Access, DMA), в обход центрального процессора. Мэппинг памяти (memory mapping) производится в аппартной среде, в обход операционной системы, что обеспечивает высокоскоростное копирование. Проблема данного метода лежит в обработке UMA (Upper Memory Area), поскольку может произойти сбой системы.

Такие подходы, в общем случае, являются неинвазивным и не требующим остановки системы, однако целостность данных не сохраняется, поскольку целевая система не останавливается и блоки данных изменяются во время процесса считывания, иными словами методы являются криминалистически сомнительными. Приоритетом в рамках поставленной задачи пользуются программные решения, поскольку с их помощью можно достигнуть схожих результатов без необходимости приобретения устройств копирования.

Аварийные дампы Microsoft, файлы гибернации. Несмотря на то, что данные методы неинвазивные и криминалистически правильные, у них мало преимуществ перед другими подходами, так как система не просто приостанавливается, но полностью завершает работу. Помимо этого, в случае аварийных дампов необходимо изменять настройки системы, а в общем случае перезагрузить систему для приведения настроек в действие.

Belkasoft Live RAM Caputer – инструмент, который может получать дампы оперативной памяти, запускаясь при этом с USB-флеш-накопителя. Данная программа использует драйверы ядра, что позволяет оперировать в привелигированом режиме. Данный метод можно классифицировать как инвазивный, криманилистически сомнительный, не требующий остановки целевой системы. Инвазивность заключается в выполнении кода программы в оперативной памяти, то же является и причиной криминалистической сомнительности. То же можно сказать о продуктах действующих по схожему принципу, таких как MANDIANT Memoryze, HBGary FastDump, FTK Imager.

### 2.2. Методы получения дампов виртуальных машин

Оперативная память виртуальных машин гипервизоров второго типа, каковым является QEMU/KVM, представляет из себя выделенный блок памяти хоста, который располагается внутри процесса QEMU. Следовательно, для получения дампов памяти виртуальны машинможно применять методы, получающие часть оперативной памяти хоста, выделенной соответсвующему процессу QEMU, либо непосредственно блок памяти, представляющий оперативную память гостевой системы.

Чтение памяти процесса. `/proc/$pid/mem` показывает содержимое отображения памяти `$pid` таким же образом, как она представлена в процессе, т. е. байт на позиции *x* в псевдо-файле тот же, что и байт на позиции *x* в процессе. Если память освобождается по определенному адресу, чтение данных по тому же адресу из соответствующего файла приведет к ошибке EIO (Input/output error). Например, поскольку первая страница процесса никогда не отображается, чтение первого байта `/proc/$pid/mem` всегда вызывает ошибку EIO. Чтобы найти корректные части памяти процесса достаточно прочитать `/proc/$pid/maps`. Данный файл содержит по одной линии на соответствующий регион памяти, что выглядит следующим образом:
```
08048000-08054000 r-xp 00000000 08:01 828061     /bin/cat
08c9b000-08cbc000 rw-p 00000000 00:00 0          [heap]
```
Первые два числа являются границами региона (адреса первого байта и байта, следующего за последним, в шестнадцатиричном формате). Далее указаны права доступа, после содержится некоторая информация о файле, если это отображение файла.

Попытка чтения `/proc/$pid/mem` из другого процесса обычным способом приводит к ошибке ESRCH (процесс не существует). Указанные права доступа для `/proc/$pid/mem` показывают больший доступ, чем, по существу, есть на самом деле. Кроме того, попытка чтения памяти процесса в то время, пока процесс ее изменяет, может дать некорректный вид памяти. Таким образом необходимо производить дополнительные операции:

* процесс, желающий читать `/proc/$pid/mem` должен присоединиться к соответствующему процессу с помощью функции `ptrace()` с флагом `PTRACE_ATTACH`. Данную процедуру выполняют отладчики при отладке процесса. По окончании чтения из `/proc/$pid/mem`, процесс должен вызвать функцию `ptrace()` с флагом `PTRACE_DETACH` для прерывания соединения.
* процесс, чья память считывается, не должен выполняться в данный момент. Обычно, вызов функции `ptrace(PTRACE_ATTACH, ...)` останавливает целевой процесс (посылает сигнал STOP), но из-за наличия состояния гонки (доставка сигнала асинхронна) вызывающий процесс должен использовать функцию `wait()`.

Метод, использующий описанный алгоритм, имплементирован в GDB (GNU Debugger) и может быть применен простым скриптом:
```
#!/bin/bash

grep rw-p /proc/$1/maps | \
sed -n 's/^\([0-9a-f]*\)-\([0-9a-f]*\) .*$/\1 \2/p' | \
while read start stop; \
do gdb --batch --pid $1 -ex "dump memory $1-$start-$stop.dump 0x$start 0x$stop"; \
done

```
Такой подход является неинвазивным и криминалистически правильным, однако требует остановки процесса, что, в свою очередь, означает остановку целевой системы.

Для улучшения данного метода возможным является дополнительное использование функции `fork()`, с целью создания копии адресного пространства процесса с механизмом копирования при записи (COW, Copy-On-Write). Однако использование функции `fork()` применительно к многопоточному процессу не является корректным, поскольку копируется единственный поток, из которого применялся вызов функции. Идея все же была осуществлена в инструменте google-coredumper, целью которого является получение дампа памяти многопоточного процесса, читаемого с помощью GDB. Функции данной библиотеки моментально останавливают все потоки, создавая копию адресного пространства процесса с механизмом копирования при записи. К сожалению, из-за сложностей реализации инструмент не оказался достаточно универсальным, с выходом новых версий ядра Linux google-coredumper перестал корректно работать. Несмотря на это, концепция метода является привлекательной, поскольку такой подход одновременно является криминалистически правильным, не требующим остановки процесса, неинвазивным и, ко всему прочему, потенциально имеет различные применения в решении ряда других задач.

Одним из инструментов, считывающих память процесса в пространстве пользователя, является CRIU (Checkpoint/Restore In Userspace) __[ССЫЛКА НА САЙТ ПРОЕКТА!]__. Используя данный инструмент можно "заморозить" работающее приложение (или его часть) и создать его контрольную точку (checkpoint) в виде набора файлов на носителе данных. Далее можно использовать данные файлы для восстановления работы приложения с того момента, когда оно было заморожено. Процедура создания контрольной точки опирается на файловую систему `/proc`, собирая, в частности, следующую информацию:

* файловые дескриптооры (`/proc/$pid/fd`, `/proc/$pid/fdinfo`);
* параметры именованых каналов;
* отображения памяти (`/proc/$pid/maps`, `/proc/$pid/map_files`).

Создание дампа производится через следующие шаги:

1. *Выявление и заморозка дерева процессов.* Идентификатор главного процесса определяется из командной строки (опция --tree). Используя этот идентификатор дампер (process dumper) проходит по директории `/proc/$pid/task/`, собирая потоки, и через `/proc/$pid/task/$tid/children` для рекурсивного сбора потомков. Во время обхода, задачи останавливаются командой ptrace с параметром PTRACE_SEIZE;
2. *Выявление ресурсов задач и создания их дампа.* На данном шаге CRIU читает информацию о выявленных задачах и записывает ее в файлы дампов. Определяются ресурсы:
  1. диапазоны виртуальных адресов через синтаксический анализ `/proc/$pid/smaps` и отображенные файлы считываются из ссылок `/proc/$pid/map_files`;
  2. номера файловых дескрипторов читаются через `/proc/$pid/fd`;
  3. дампы основных параметров задач (такие как регистры) создаются с помощью ptrace() и синтаксического анализа записи `/proc/$pid/stats`.
Далее CRIU вводит код-паразит (parasite code) через ptrace интерфейс. Через него CRIU получает дополнительную информацию, такую как права доступа, содержание памяти;
3. *Очистка.* После того, как созданы дампы, снова используется `ptrace()` для выгрузки кода-паразита и восстановления оригинального кода. Далее CRIU отсоединяется от задач и они продолжают работу.

Оставим описание процедуры восстановления, поскольку она не относится напрямую к цели данной работы.

При создании дампа также существует возможность не останавливать задачи. Это может привести к тому, что восстановление с момента заморозки окажется невозможным, тем не менее дампы будут присутствовать.

Предложенные методы получения дампов памяти, несомненно, дополнительно решают широкий спектр других задач. Однако, исходя из описания балунинга памяти гл. 1, содержание дампа, соответствующего области памяти, выделенной гостевой системе, может оказаться некорректным, а именно, дамп будет иметь больший размер. В этом случае появляется дополнительная задача для анализа дампов памяти, поскольку несмотря на искаженность полученных данных, информация о реальной адресации присутствует в памяти процесса.

Наиболее практичным методом получения дампа оперативной памяти представляется использование средств гипервизора. Мониторы виртуальных машин среди стандартных функций, как правило, имеют функции сохранения состояния виртуальной машины и создания дампов оперативной памяти гостевой системы. В качестве примера команды  сохранения состояния можно привести команду savevm рассматриваемого гипервизора QEMU/KVM. Общий алгоритм работы представляет из себя набор последовательных операций:

* приостановка работы гостевой операционной системы;
* копирование блоков памяти, сопряженное с наложениями блокировок и вычеслениями дельты, т. е. разности между текущим и предыдущим состоянием виртуальной машины;
* восстановление работы гостевой операционной системы, что далеко не во всех случаях означает тривиальную "разморозку" виртуальной машины.

Практически все методы получения дампа оперативной памяти с помощью гипервизора сводятся к набору вышеперечисленных операций, среди которых важной является приостановка работы гостевой операционной системы, что обеспечивает целостность считываемх данных. Это обуславливается тем, что основной целью таких функций является либо сохранение состояния виртуальной машины для последующего восстановления, либо для анализа содержимого ее оперативной памяти. Таким образом, такие методы можно объединить в одну группу криминалистически правильных, неинвазивных, требующих остановки целевой системы. Кроме того стоит заметить, что использование конкретных функций одного гипервизора является далеко не лучшим решением по критерию универсальности, поэтому следует принять в рассмотрение более общие подходы.

## Глава 3. Реализованный метод

Универсальной библиотекой для работы с различными гипервизорами (QEMU/KVM, Xen, VirtualBox, Microsoft Hyper-V и т. д.) является libvirt. Целью библиотеки libvirt является предоставление унифицированного интерфейса локального или удаленного управления *доменами узлов*. Под узлом здесь подразумевается отдельная физическая машина, под доменом - отдельная операционная система, работающая на виртуальной машине. Таким образом, libvirt предоставляет все функции, необходимые для управления: создание, изменение, мониторинг, контроль, миграция и остановка доменов - все в пределах поддержки данных операций гипервизором.

Общий алгоритм работы реализуемого метода содержит стандартный набор операций, применяемых для получения дампов оперативной памяти. Рассмотрим подробно последовательность операций для формирования четкого представления о методе и возможности его применения на практике.

1. Создание хранилища дампа. В зависимости от решаемой задачи дамп оперативной памяти гостевой машины можно хранить в:

  * Энергозависимой памяти (оперативной памяти);
  * Энергонезависимой памяти (персистентном носителе данных).

   Преимущества и недостатки использования разных типов памяти будут наглядно продемонстрированы в следующей главе.

   Как правило, запись и хранение дампа в оперативной памяти дает большое преимущество в плане быстродействия алгоритма, однако предполагает наличие большого ее количества. Для выделения достаточно большого блока памяти используется комбинация функций `shm_open()`, `mmap()` и `ftruncate()` для создания файлового дескриптора, ссылающегося на область памяти, выделения область памяти и ее инициализации соответственно. При этом выделенный блок памяти остается зарезервированным даже после завершения процесса, таким образом на пользователя ложится ответственность за своевременное освобождение памяти, что осуществляется вызовом функции `shm_unlink()`.

   Запись дампа в энергонезависимую память дает преимущество его долговременного хранения. Оптимальным вариантом для записи в данном случае является использование функции `open()`, аналогом которого является вышеупомянутая функция `shm_open()`. Функция `open()` создает файловый дескриптор ссылающийся на файл, хранящийся в энергонезависимой памяти. Таким образом, отличие от случая с оперативной памятью заключается в более длительном доступе к энергонезависимой памяти.

   Для обоих случаев выделение региона памяти осуществляется с помощью функции mmap(), поскольку это обеспечивает оптимальный вариант при работе с большими массивами данных. Причиной этому служит низкая гранулярность применяемых в данном случае операций, что уменьшает число обращений к операционной системе. Другой важной причиной является то, что операционная система оптимизированно обрабатывает запросы на чтение и запись, применяя операции только в случае необходимости.

2. Копирование блока памяти. Основной функцией на этом шаге является virDomainMemoryPeek(), принимающей в качестве аргументов указатель на домен, значение которого косвенно определяется пользователем, стартовый адрес блока читаемой памяти, его размер, адрес буфера, куда записывется результат, а также флаг, отвечающий за тип читаемой памяти. Адрес домена определяется одной из функций, принимающих в качестве аргумента один из однозначно определенных параметров домена: идентификационный номер, имя или UUID (Universally Unique Identifier). Стартовый адрес блока читаемой памяти может быть интерпретируем по-разному, в зависимости от значения флага:

  * VIR_MEMORY_VIRTUAL - указанный адрес блока читаемой памяти является адресом в виртуальном адресном пространстве гостевой операционной системы;
  * VIR_MEMORY_PHYSICAL - указанный адрес блока читаемой памяти является физическим адресом.

   В данной задаче предпочтительно выбрать значение VIR_MEMORY_PHYSICAL, поскольку использование виртуальных адресов сводится к их трансляции в соответсвующий физический адрес, что означает привлечение дополнительных операций. Кроме того, значения необходимых физических адресов в случае снятия полного дампа тривиальны (стартовый адрес 0).

   Далее подробно рассмотрим работу функции `virDomainMemoryPeek()`, разделив этап на несколько более простых шагов, поскольку в данном случае копирование производится нетривиальным способом.

  1. Установка блокировки. Используя функцию `rcu_read_lock()` на читаемый регион памяти устанавливается блокировка, основывающаяся на механизме read-copy-update (RCU). Ключевое свойство такой блокировки заключается в том, что потоки, производящие чтение, могут получать доступ к данным, даже если происходит их обновление. Такой эффект достигается за счет дополнительных затрат на сохранение старой версии данных. Так как достаточно значительную часть оперативной памяти составляют статичные структуры, такая блокировка не несет неприемлемых затрат, при этом она обеспечивает целостность считываемых данных.

  2. Формирование и отправка сообщения гипервизору. Основную часть управления библиотека libvirt производит через вызов соответствующих функций гипервизора. Для этого необходимая команда представляется в виде, присущем гипервизору, например, в случае QEMU/KVM используется QMP (QEMU Machine Protocol ), базирующийся на формате JSON. Далее, используя формат сериализации External Data Representation (XDR), данная команда передается гипервизору в виде сообщения, где после процедуры десериализации сообщения гипервизор выполняет необходимые действия.

  3. Выполнение команды гипервизором. В рассматриваемом случае выполняется команда pmemsave. Данная команда, обработанная в главном цикле обработки событий (см. гл. 1), вызывает соответствующую функцию `qmp_pmemsave()`. Сначала производится поиск соответствующего блока памяти процесса, откуда необходимо произвести чтение, что сопровождается трансляцией виртуального адреса в физический адрес. Далее производится чтение из блока памяти в буфер при помощи функции `memcpy()`, которая имеет свойство потоковой безопасности по стандарту POSIX, или иными словами имеет свойство реентерабельности. Это в свою очередь означает, что не обрабатываются случаи обновления данных во время их чтения. В случае использования данной функции непосредственно гипервизором, такая ситуация учитывается путем приостановки выполнения кода виртуальной машины с помощью глобальной блокировки, в приведенном же случае данную проблему устраняет установленная ранее блокировка.

  4. Завершение этапа. Результат возвращается в тело функции `virDomainMemoryPeek()` аналогично отправке сообщения гипервизору, после чего копируется в указанный буфер. Процесс завершается снятием блокировки функцией `rcu_read_unlock()` и освобождением ресурсов.

## Глава 4.

### Постановка эксперимента

Цель эксперимента - показать состоятельность метода применительно к решению поставленных задач и обозначить наиболее важные характеристики реализованного метода. В связи с этим в эксперимент необходимо включить следующие пункты:

* проверка корректности метода, означающая соответсвие требованиям криминалистически правильных или криминалистически сомнительных методов и характеризующаяся наличием этапов:

  * проверка целостности копируемых блоков памяти;
  * проверка корректности полученного дампа памяти оперативной памяти;

* выявление преимуществ и недостатков реализованного метода в сравнении со стандартнымы методами получения дампов, главным образом, подхода, реализованного функцией pmemsave гипервизора QEMU/KVM.

## Список литературы

1. Thu Yein Win, Huaglory Tianfield, Quentin Mair. Virtualization Security Combining Mandatory Access Control and Virtual Machine Introspection. UCC '14: Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing. December 2014, P. 1004.
2. Федотов Н.Н. Форензика – компьютерная криминалистика. Москва: «Юридический мир», 2007. 360 с.

3. http://qemu.weilnetz.de/qemu-tech.html#QEMU-Internals

4. Naja Davis. Live Memory Acquisition for Windows Operating Systems:  Cover Page and Abstract Tools and Techniques for Analysis. Eastern Michigan University, p. 4–6

5. http://www.linux-kvm.org/page/Memory

6. Yasunori Goto. Kernel-based Virtual Machine Technology. FUJITSU Sci. Tech. J., Vol. 47, No. 3, July 2011. pp. 362–368

7. Brian D. Carrier, Joe Grand. A hardware-based memory acquisition procedure for digital investigations. Digital Investigation (2004) 1, p 50-60

>Для доказательства что реализован криминалистичски правильный метод (фактически нет "криминалистически сомнительного", наверное).
Rodney McKemmish. When is Digital Evidence Forensically Sound? Advances in Digital Forensics IV. IFIP — The International Federation for Information Processing Volume 285, 2008, pp 3-15
